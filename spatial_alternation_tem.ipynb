{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "245c2f04-a681-450d-b5f7-b64d5141e34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import glob, os, shutil\n",
    "import importlib.util\n",
    "# Own module imports\n",
    "import world\n",
    "import utils\n",
    "import parameters\n",
    "import model as model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085e794c-7599-4f64-8180-8950363268a3",
   "metadata": {},
   "source": [
    "## I think that global path equivalence reflects states in which subsequent states have the same or very similar sensory stimuli.\n",
    "\n",
    "### This should be more common in exploration, because reward, a sensory stimulus, is common no matter what arm the animal goes to.\n",
    "\n",
    "### During alternation, depending on the animal's latent state (previous two arms) certain transitions should lead to different sensory outcomes, even when passing through the same location (going left vs right depending on the previous outer arm should lead to different predictions about the upcoming reward state)\n",
    "\n",
    "#### A problem with the above is that, at perfect performance, almost all trajectories will be rewarded. So they should also show global path equivalence. \n",
    "\n",
    "#### The most striking and telling difference may be before perfect performance, since the animal will do bad trajectories that should predict differing rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14948233-baa1-4397-9f8f-94768c0fd5cc",
   "metadata": {},
   "source": [
    "## Let's accept the hypothesis. How would one verify this with TEM? As is, TEM uses discrete stimuli, and there is no notion of stimulus similarity. Because of this, different arms of the track would have totally different stimuli and therefore no path equivalence can be identified\n",
    "\n",
    "### I can think of two ways to deal with this. First and most simply, I can add a state at the end of each rewarded transition that solely contains reward (similar to how they create a new reward state at the end of every 4 laps in the tonegawa track)\n",
    "\n",
    "### I can also think about augmenting TEM to predict tuples of sensory stimuli instead of unitary values. The simplest version of this would look like this: (visual, reward). Would this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7ff3a-de47-43dd-9925-1a563a7dc748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
